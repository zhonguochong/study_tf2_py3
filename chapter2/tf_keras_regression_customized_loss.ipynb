{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.3\n",
      "numpy 1.18.1\n",
      "pandas 1.0.1\n",
      "sklearn 0.21.0\n",
      "tensorflow 2.1.0\n",
      "tensorflow_core.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd,  sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3870, 8) (3870,)\n",
      "(11610, 8) (11610,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data, housing.target, random_state =7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_all, y_train_all, random_state =11)\n",
    "\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def customized_mse(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape = x_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "model.summary()\n",
    "model.compile(loss = customized_mse,\n",
    "              optimizer = 'sgd',\n",
    "              metrics = ['mean_squared_error'])\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "    patience=5, min_delta=1e-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 1.1434 - mean_squared_error: 1.1434 - val_loss: 0.8152 - val_mean_squared_error: 0.8152\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 1.1462 - mean_squared_error: 1.1462 - val_loss: 0.6598 - val_mean_squared_error: 0.6598\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.4896 - mean_squared_error: 0.4896 - val_loss: 0.4611 - val_mean_squared_error: 0.4611\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.4195 - mean_squared_error: 0.4195 - val_loss: 0.4244 - val_mean_squared_error: 0.4244\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3964 - mean_squared_error: 0.3964 - val_loss: 0.4108 - val_mean_squared_error: 0.4108\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3838 - mean_squared_error: 0.3838 - val_loss: 0.3932 - val_mean_squared_error: 0.3932\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3801 - mean_squared_error: 0.3801 - val_loss: 0.3894 - val_mean_squared_error: 0.3894\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3701 - mean_squared_error: 0.3701 - val_loss: 0.3852 - val_mean_squared_error: 0.3852\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3645 - mean_squared_error: 0.3645 - val_loss: 0.3776 - val_mean_squared_error: 0.3776\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3639 - mean_squared_error: 0.3639 - val_loss: 0.3800 - val_mean_squared_error: 0.3800\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3633 - mean_squared_error: 0.3633 - val_loss: 0.3764 - val_mean_squared_error: 0.3764\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.3572 - mean_squared_error: 0.3572 - val_loss: 0.3726 - val_mean_squared_error: 0.3726\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3546 - mean_squared_error: 0.3546 - val_loss: 0.3685 - val_mean_squared_error: 0.3685\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3624 - mean_squared_error: 0.3624 - val_loss: 0.3764 - val_mean_squared_error: 0.3764\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train,\n",
    "                    epochs = 100,\n",
    "                    validation_data = [x_valid_scaled, y_valid],\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize = (8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "    \n",
    "plot_learning_curves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
